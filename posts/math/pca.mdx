---
title: PCA
description: 주성분 분석, Principal Component Analysis
cover:
tags: math, pca
date: 2025-08-18
author: yeahx4
series: 
seriesIndex: -1
coverTitle: PCA
coverSub: 
coverTs: 120
coverBg: E06577
coverColor: ffffff
---

# 차원의 저주

많은 데이터는 고차원 공간에 존재합니다. 즉, 굉장히 많은 feature로 이루어져 있습니다.
하지만, 고차원에서는 우리의 직관과는 다른 현상이 발생합니다. 예를 들어, 2D의
단위면적에서 임의로 두 점을 찍었을 때, 두 점 사이의 거리의 기댓값은 다음처럼 구할 수 있습니다.

```math
\mathbb{E}[d] = \int_0^1 \int_0^1 \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} \, dx_1 \, dx_2 \, dy_1 \, dy_2
```

이 식은 수학적 기교로 계산할 수 있긴 하지만 고차원으로 확장하기 어려우니 파이썬을 통해 통계적으로
근삿값을 구하겠습니다. 몬테-카를로 방법이라고도 합니다.

```py
import numpy as np

def dist(n, trials=1_000_000, seed=None):
    rng = np.random.default_rng(seed)
    A = rng.random((trials, n))
    B = rng.random((trials, n))
    return np.linalg.norm(A - B, axis=1).mean()
```

```py
print(dist(2, seed=42)) # 0.521109890202126
```

대략 0.52 정도의 값이 나옵니다. 차원이 커진다면 어떨까요?

```py
import matplotlib.pyplot as plt

dims = range(2, 101)
values = [dist(n, seed=42, trials=10_000) for n in dims]

plt.figure(figsize=(8,5))
plt.plot(dims, values)
plt.xlabel("Dimension (n)")
plt.ylabel("Expected Distance")
plt.title("Monte Carlo Estimate of $E[d]$ in $[0,1]^n$")
plt.grid(True)
plt.show()
```

![Expected distance](/img/math/pca/dist-avg.png)

점점 점들 사이의 평균 거리가 멀어지는 것을 볼 수 있습니다. 1,000,000 차원에서는 평균 거리가
408 정도로 매우 커집니다. 즉, 차원이 클 수록 데이터가 멀리 퍼져 희소(sparse)해집니다.
이렇게 굉장히 많은 feature는 모델의 학습을 느리게 할 뿐 아니라 좋은 솔루션을 찾기 힘들게
만듭니다. 이를 차원의 저주(curse of dimensionality)라고 합니다. 이 문제를 해결하는 한
방법은 데이터를 추가하는 것입니다. 하지만, 차원이 많을 때는 필요한 데이터도 기하급수적으로
늘어납니다. 다른 방법으로는 차원 자체를 줄일 수 있습니다. 많은 경우 모든 차원이 필요하지 않습니다.
몇 차원을 제거하더라도 데이터에 큰 손실이 없는 경우가 많습니다. 이것을
차원 축소(dimensionality reduction)라고 하고 차원 축소의 대표적인 기법이 PCA입니다.

# PCA

차원 축소는 여러 방법으로 할 수 있습니다. PCA는 투영(projection)을 사용해서 차원을
축소합니다. 아래 함수로 만들어진 데이터가 있다고 해 봅시다.

```py
angle = np.pi / 5
stretch = 5
m = 200

np.random.seed(3)
X = np.random.randn(m, 2) / 10
X = X @ np.array([[stretch, 0], [0, 1]])
X = X @ [[np.cos(angle), np.sin(angle)],
        [np.sin(angle), np.cos(angle)]]
```

![Scatter plot](/img/math/pca/scatter.png)

$m = 200$개의 데이터를 만들어 $x_1$ 방향으로 늘린 후 회전 행렬을 통해 $\pi / 5$만큼
돌려 주었습니다. 각 데이터는 $x_1, x_2$로 표현되는 2차원 데이터입니다. 그런데 2차원 보단
1차원 데이터처럼 생겼습니다. 저희의 목표는 이 2차원 데이터를 1차원으로 축소하는 것입니다.
아까 PCA는 투영을 사용한다고 했습니다. 그런데, 어디다 투영해야 할까요? 아래 그래프는 다양한
축을 기준으로 투영한 결과입니다.

![Scatter plot](/img/math/pca/proj.png)

위 그래프를 보면 어떤 축 $z_1$을 잡고 그 위에 데이터들을 투영했습니다. 어느 방향으로 투영하느냐에
따라 데이터가 멀리 퍼져있기도 하고 거의 한 점에 모여있기도 합니다. 다르게 표현하면 데이터가
흩어져 있다는 것은 분산이 크다는 것이고, 한 점에 모여있다는 것은 분산이 작다는 것입니다.
차원 축소의 목적은 차원을 줄이되, 최대한 정보의 손실을 줄이는 것입니다. 위 그래프에서
세 번째 축의 경우 모든 데이터가 거의 한 점에 모여 굉장히 많은 정보가 손실되었습니다.
즉, 분산을 가장 크게 하는 축을 찾아서 그 축으로 투영하면 정보의 손실을 최소화하면서 차원을
줄일 수 있습니다. 이것이 PCA의 핵심 아이디어입니다.

## 작동 방식

분산을 최대화하는 축을 찾는다는 것을 다르게 말하면 원본 데이터셋과 투영된 데이터셋 사이의
평균 제곱 오차(MSE)를 최소화하는 것입니다. 이렇게 첫 축을 찾았으면 다음으로 이 축과 직교하고
분산을 최대화하는 두 번째 축을 찾습니다. 다음 축은 두 축과 모두 직교하면서 분산을 최대화하는
축을 찾습니다. 이 과정을 반복해서 원하는 차원만큼 축을 찾습니다. 이때 각 축을
주성분(principal component, PC)라고 합니다.

그럼 주성분을 어떻게 구할 수 있을까요? PCA의 주성분은 교윳값 분해(eigendecomposition) 또는
SVD(특이값 분해)로 구할 수 있습니다. 교윳값 분해는 square matrix에 대해서만 정의되므로,
일반적으로 SVD를 사용합니다.